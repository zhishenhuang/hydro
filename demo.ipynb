{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa909aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faf344839f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os,sys\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "import models\n",
    "sys.path.insert(0,'/home/huangz78/hydro/unet3d/')\n",
    "import torch.fft as F\n",
    "from models.unet3d_model import UNet3D, ResidualUNet3D \n",
    "from models.dnet import weights_init, Discriminator\n",
    "import utils\n",
    "from utils import *\n",
    "# from train import gan_train,noise_generate\n",
    "from importlib import reload\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4fb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693500b",
   "metadata": {},
   "source": [
    "# track adversial training errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "errorRec_path = '/mnt/DataA/checkpoints/leo/hydro/wgan_train_track_Abel-gaussian-double.npz'\n",
    "errordata = np.load(errorRec_path)\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "print(errordata.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2400b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(errordata['nrmse_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f38175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualization(errordata,errordata.files,log=False,figsize=(10,35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108fee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n",
      "Total amount of available files: 13798\n",
      "Train file amount: 1000\n",
      "Val   file amount: 100\n",
      "Test  file amount: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22383/967361296.py:20: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  trainfiles = random.sample(set(ncfiles),k=traintotal)\n",
      "/tmp/ipykernel_22383/967361296.py:22: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  valfiles  = random.sample(ncfiles,k=valtotal)\n",
      "/tmp/ipykernel_22383/967361296.py:24: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  testfiles = random.sample(ncfiles,k=testtotal)\n"
     ]
    }
   ],
   "source": [
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "datapath = '/mnt/DataB/hydro_simulations/data/'\n",
    "\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".nc\"):\n",
    "        ncfiles.append(file)\n",
    "traintotal = 1000\n",
    "valtotal   = 100\n",
    "testtotal  = 300\n",
    "print('Total amount of available files:', len(ncfiles))\n",
    "print('Train file amount: {}'.format(traintotal))\n",
    "print('Val   file amount: {}'.format(valtotal))\n",
    "print('Test  file amount: {}'.format(testtotal))\n",
    "\n",
    "trainfiles = random.sample(set(ncfiles),k=traintotal)\n",
    "ncfiles = set(ncfiles) - set(trainfiles)\n",
    "valfiles  = random.sample(ncfiles,k=valtotal)\n",
    "ncfiles = set(ncfiles) - set(valfiles)\n",
    "testfiles = random.sample(ncfiles,k=testtotal)\n",
    "np.savez('/mnt/DataA/checkpoints/leo/hydro/' +f'filesUsed.npz',\\\n",
    "                 trainfiles=trainfiles,valfiles=valfiles,testfiles=testfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbdf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_(netG, testfiles,\\\n",
    "          batchsize=5,dep=8,img_size=320,\\\n",
    "          noise_mode='Abel-gaussian',\\\n",
    "          normalize_factor=50,\\\n",
    "          volatility=.05,sigma=2,xi=.02,scaling=1,white_noise_ratio=1e-4,device=torch.device('cpu'),\\\n",
    "          resize_option=False):\n",
    "    testfile_num = len(testfiles)\n",
    "    batchsize = min(testfile_num,batchsize)\n",
    "    # set the model in eval mode\n",
    "    netG.to(device)\n",
    "    netG.eval()\n",
    "    Mass_diff = np.zeros(len(testfiles)); nrmse = np.zeros(len(testfiles)); nl1err = np.zeros(len(testfiles))\n",
    "\n",
    "    # evaluate on validation set\n",
    "    fileind = 0\n",
    "    batch_step = 0\n",
    "    with torch.no_grad():\n",
    "        while fileind < testfile_num:            \n",
    "            dyn, noise = load_data_batch(fileind,testfiles,b_size=batchsize,dep=dep,img_size=img_size,\\\n",
    "                                        resize_option=resize_option,\\\n",
    "                                        noise_mode=noise_mode,normalize_factor = normalize_factor,\\\n",
    "                                        volatility=volatility,sigma=sigma,xi=xi,scaling=scaling,\\\n",
    "                                         white_noise_ratio=white_noise_ratio)\n",
    "            real_cpu = dyn.to(device)\n",
    "            noise    = noise.to(device)\n",
    "            fake = netG(noise + real_cpu).clamp(min=0).detach()\n",
    "            fake[real_cpu==0] = 0\n",
    "\n",
    "            mass_fake = compute_mass(fake,device=device)\n",
    "            mass_real = compute_mass(real_cpu,device=device)\n",
    "            mass_diff = torch.divide(torch.abs(mass_fake - mass_real), mass_real).sum()/dep\n",
    "            for ind in range(batchsize):\n",
    "                Mass_diff[fileind+ind] = mass_diff\n",
    "                nrmse[fileind+ind]     = aver_mse(fake[ind:ind+1,:,:,:,:],real_cpu[ind:ind+1,:,:,:,:])\n",
    "                nl1err[fileind+ind]    = aver_l1(fake[ind:ind+1,:,:,:,:],real_cpu[ind:ind+1,:,:,:,:])\n",
    "            del dyn, real_cpu, noise \n",
    "            fileind += batchsize\n",
    "            print(f'[{fileind}/{testfile_num}]')\n",
    "    return Mass_diff, nrmse, nl1err # Mass_diff/(testfile_num*self.dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25db9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(testfiles,\\\n",
    "          batchsize=5,dep=8,img_size=320,\\\n",
    "          noise_mode='Abel-gaussian',\\\n",
    "          normalize_factor=50,\\\n",
    "          volatility=.05,sigma=2,xi=.02,scaling=1,white_noise_ratio=1e-4,device=torch.device('cpu'),\\\n",
    "          resize_option=False,\\\n",
    "          lr=1e-5,weight_masscon=1e2,weight_TVA=1e-4,maxIter=5e3):\n",
    "    testfile_num = len(testfiles)\n",
    "    batchsize = min(testfile_num,batchsize)\n",
    "    \n",
    "    Mass_diff = np.zeros(len(testfiles)); nrmse = np.zeros(len(testfiles)); nl1err = np.zeros(len(testfiles))\n",
    "    # evaluate on validation set\n",
    "    fileind = 0\n",
    "    batch_step = 0\n",
    "    with torch.no_grad():\n",
    "        while fileind < testfile_num:\n",
    "            print(f'Current iter: [{fileind}/{testfile_num}]')\n",
    "            dyn, noise = load_data_batch(fileind,testfiles,b_size=batchsize,dep=dep,img_size=img_size,\\\n",
    "                                            resize_option=resize_option,\\\n",
    "                                            noise_mode=noise_mode,normalize_factor=normalize_factor,\\\n",
    "                                            volatility=volatility,sigma=sigma,xi=xi,scaling=scaling,\\\n",
    "                                            white_noise_ratio=white_noise_ratio)\n",
    "            real_cpu = dyn.to(device)\n",
    "            noise    = noise.to(device)\n",
    "            noisy_sg = noise + real_cpu\n",
    "            \n",
    "            truemass = compute_mass(real_cpu,device=device)\n",
    "            denoised_dyn = postprocessor(noisy_sg, truemass,\\\n",
    "                                         lr=lr,\\\n",
    "                                         weight_datafid=0, weight_masscon=weight_masscon, weight_TVA=weight_TVA,\\\n",
    "                                         dyn=real_cpu,\\\n",
    "                                         maxIter=maxIter,\\\n",
    "                                         verbose=False,device=device)\n",
    "\n",
    "            mass_fake = compute_mass(denoised_dyn,device=device)\n",
    "            mass_real = compute_mass(real_cpu,device=device)\n",
    "            mass_diff = torch.divide(torch.abs(mass_fake - mass_real), mass_real).sum()/dep\n",
    "            for ind in range(batchsize):\n",
    "                Mass_diff[fileind+ind] = mass_diff\n",
    "                nrmse[fileind+ind]     = aver_mse(denoised_dyn[ind:ind+1,:,:,:,:],real_cpu[ind:ind+1,:,:,:,:])\n",
    "                nl1err[fileind+ind]    = aver_l1(denoised_dyn[ind:ind+1,:,:,:,:],real_cpu[ind:ind+1,:,:,:,:])\n",
    "            del dyn, real_cpu, noise \n",
    "            fileind += batchsize\n",
    "            \n",
    "    return Mass_diff, nrmse, nl1err # Mass_diff/(testfile_num*self.dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a G net\n",
    "gnet = ResidualUNet3D(1,1,num_levels=4,is_segmentation=False,final_sigmoid=False)\n",
    "# gpath = '/mnt/DataA/checkpoints/leo/hydro/netG_wg_Abel-gaussian_epoch_3.pt'\n",
    "# gpath = '/mnt/DataA/checkpoints/leo/hydro/netG_wg_Abel-gaussian_scaling_0.3_supweigtdecay_0.97_epoch_6.pt'\n",
    "gpath = '/mnt/DataA/checkpoints/leo/hydro/netG_wg_Abel-gaussian-double_epoch_3.pt'\n",
    "checkpoint = torch.load(gpath)\n",
    "gnet.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "gnet.eval()\n",
    "print(f' G net is successfully loaded from {gpath}! ')\n",
    "gnet_params_num = gnet.n_params\n",
    "print('total amount of parameters in gnet: ', gnet_params_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dae51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise_mode = 'Abel-gaussian-double'\n",
    "scaling = 1\n",
    "massdiff, nrmse, nl1err = \\\n",
    "test_(gnet,testfiles,noise_mode=noise_mode,scaling=scaling,device=torch.device('cuda:0'))\n",
    "dir_rec = f'/home/leo/hydro/hist_{noise_mode}_{scaling}.npz'\n",
    "np.savez(dir_rec,massdiff=massdiff,nrmse=nrmse,nl1err=nl1err)\n",
    "print(f'test result saved for noise mode {noise_mode}, scaling {scaling}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(nrmse)\n",
    "plt.title(f'l2err mean={nrmse.mean():.4f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3452240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iter: [0/300]\n",
      "> \u001b[0;32m/home/leo/hydro/utils.py\u001b[0m(449)\u001b[0;36mpostprocessor\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    447 \u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iter {t:4d}, loss total {loss.data:5f}, data fid. {data_fidelity:5f}, mass fid. {weight_masscon * mass_fidelity:5f}, TVA {weight_TVA * tva:5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    448 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 449 \u001b[0;31m        \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    450 \u001b[0;31m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    451 \u001b[0;31m        \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p dyn_new.requires_grad\n",
      "True\n",
      "ipdb> dyn_var = torch.norm(dyn_new)\n",
      "ipdb> p dyn_var.requires_grad\n",
      "False\n",
      "ipdb> newvar = dyn_new + 5\n",
      "ipdb> p newvar.requires_grad\n",
      "False\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22383/1439222862.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmassdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnl1err\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     \u001b[0mtest_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdir_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/home/leo/hydro/hist_{noise_mode}_{scaling}_baseline.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_rec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmassdiff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmassdiff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnrmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrmse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnl1err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnl1err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22383/518571328.py\u001b[0m in \u001b[0;36mtest_baseline\u001b[0;34m(testfiles, batchsize, dep, img_size, noise_mode, normalize_factor, volatility, sigma, xi, scaling, white_noise_ratio, device, resize_option, lr, weight_masscon, weight_TVA, maxIter)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtruemass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_cpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             denoised_dyn = postprocessor(noisy_sg, truemass,\\\n\u001b[0m\u001b[1;32m     29\u001b[0m                                          \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                          \u001b[0mweight_datafid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_masscon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_masscon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_TVA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_TVA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hydro/utils.py\u001b[0m in \u001b[0;36mpostprocessor\u001b[0;34m(noisy_dyn, truemass, lr, maxIter, weight_datafid, weight_masscon, weight_TVA, print_every, dyn, device, verbose)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iter {t:4d}, loss total {loss.data:5f}, data fid. {data_fidelity:5f}, mass fid. {weight_masscon * mass_fidelity:5f}, TVA {weight_TVA * tva:5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hydro/utils.py\u001b[0m in \u001b[0;36mpostprocessor\u001b[0;34m(noisy_dyn, truemass, lr, maxIter, weight_datafid, weight_masscon, weight_TVA, print_every, dyn, device, verbose)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iter {t:4d}, loss total {loss.data:5f}, data fid. {data_fidelity:5f}, mass fid. {weight_masscon * mass_fidelity:5f}, TVA {weight_TVA * tva:5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "noise_mode = 'Abel-gaussian'\n",
    "scaling = 1\n",
    "massdiff, nrmse, nl1err = \\\n",
    "                    test_baseline(testfiles,noise_mode=noise_mode,scaling=scaling,device=torch.device('cuda:0'))\n",
    "dir_rec = f'/home/leo/hydro/hist_{noise_mode}_{scaling}_baseline.npz'\n",
    "np.savez(dir_rec,massdiff=massdiff,nrmse=nrmse,nl1err=nl1err)\n",
    "print(f'test result saved for noise mode {noise_mode}, scaling {scaling}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3107e",
   "metadata": {},
   "source": [
    "# check denoised slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a55225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a G net\n",
    "# gnet = UNet3D(1,1,is_segmentation=False,final_sigmoid=False)\n",
    "gnet = ResidualUNet3D(1,1,num_levels=4,is_segmentation=False,final_sigmoid=False)\n",
    "gpath = '/mnt/DataA/checkpoints/leo/hydro/netG_wg_Abel-gaussian-double_epoch_3.pt'\n",
    "# # gpath = '/home/huangz78/checkpoints/netG_warmup_masspenned.pt'\n",
    "checkpoint = torch.load(gpath)\n",
    "gnet.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "gnet.eval()\n",
    "print(f' G net is successfully loaded from {gpath}! ')\n",
    "gnet_params_num = gnet.n_params\n",
    "print('total amount of parameters in gnet: ', gnet_params_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496594b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnet = Discriminator(ndf=4,sigmoid_on=True,imgsize=(8,320,320))\n",
    "dpath = '/mnt/DataA/checkpoints/leo/hydro/netD_wg_Abel-linear_epoch_6.pt'\n",
    "checkpoint = torch.load(dpath)\n",
    "dnet.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "\n",
    "dnet_params_num = dnet.n_params\n",
    "print('total amount of parameters in dnet: ', dnet_params_num )# dnet.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnet_params_num/dnet_params_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d4d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapath = '/mnt/DataB/hydro_simulations/data/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".nc\"):\n",
    "        ncfiles.append(file)\n",
    "print('Total amount of files:', len(ncfiles))\n",
    "img_size = 320\n",
    "dep = 8\n",
    "# fileexp_ind = 5636 # type 1\n",
    "fileexp_ind = 12273 # type 2\n",
    "# fileexp_ind = np.random.randint(len(ncfiles))\n",
    "print(f'current file index: {fileexp_ind}')\n",
    "# for fileexp_ind in range(len(ncfiles)):\n",
    "filename = ncfiles[fileexp_ind]\n",
    "sim = xr.open_dataarray(datapath+filename)\n",
    "sim.close()\n",
    "time_pts = torch.round(torch.linspace(0,40,dep)).int() \n",
    "# subgroup = 0\n",
    "dyn   = torch.zeros((1,1,dep,img_size,img_size))    # load one sample, show it\n",
    "noise = torch.zeros((1,1,dep,img_size,img_size))    \n",
    "for t in range(dep):\n",
    "    dyn[0,0,t,:,:] = torch.tensor( sim.isel(t=time_pts[t])[:img_size,:img_size].values )\n",
    "#     dyn[0,0,t,:,:] = torch.tensor( sim.isel(t=t+dep*subgroup)[:img_size,:img_size].values )\n",
    "# for t in range(dep):\n",
    "#     dyn[0,0,t,:,:] = resize(sim.isel(t=t)[:img_size,:img_size].values,(256,256),anti_aliasing=True)\n",
    "\n",
    "normalize_factor   = 50\n",
    "dyn = dyn.clamp(max=normalize_factor)\n",
    "dyn[0,0,:,:,:]  = dyn[0,0,:,:,:] / normalize_factor\n",
    "\n",
    "noise = torch.zeros(dyn.shape) # make a noise sample, add it to the ground truth, show it\n",
    "# for t in range(dep): # different noise for each frame when using a 'for' loop\n",
    "#     noise[0,0,t,:,:] = noise_generate(dyn[0,0,t,:,:],mode='linear',scaling=dyn.max().numpy()) \n",
    "#     noise[0,0,t,:,:] = noise_generate(dyn[0,0,t,:,:],mode='const_rand',scaling=dyn.max().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3a8a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise[0,0,:,:,:] = noise_generate(dyn[0,0,:,:,:],mode='Abel-gaussian-double',\\\n",
    "                                  sigma=2,volatility=.05,xi=.02,scaling=1,\\\n",
    "                                  abel_method='basex',white_noise_ratio=1e-4) \n",
    "\n",
    "# noise[dyn==0] = 0\n",
    "noisy_dyn = dyn + noise\n",
    "%matplotlib inline\n",
    "\n",
    "vmax_val = .5\n",
    "illustrate(dyn, vmin=0, vmax=vmax_val,title='ground truth dynamics',time_pts=time_pts)\n",
    "illustrate(noise.abs(), vmin=0, vmax=vmax_val,title='noise - absolute value',time_pts=time_pts)\n",
    "illustrate(noisy_dyn, vmin=noise.min(), vmax=vmax_val,title='noisy dynamics',time_pts=time_pts)\n",
    "illustrate(torch.divide(noise.abs(),dyn),vmin=0, vmax=1,title='relative L1 error',time_pts=time_pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ec2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply denoiser, show the outcome\n",
    "noisy_dyn = noise + dyn\n",
    "denoised_dyn = gnet(noisy_dyn).clamp(min=0).detach()\n",
    "denoised_dyn[dyn==0] = 0\n",
    "print('un-denoised L2 err: ', (torch.norm(noisy_dyn - dyn)/torch.norm(dyn)).item())\n",
    "print('denoised    L2 err: ', (torch.norm(denoised_dyn - dyn)/torch.norm(dyn)).item())\n",
    "print('un-denoised L1 err: ', (torch.norm(noisy_dyn - dyn,p=1)/torch.norm(dyn,p=1)).item())\n",
    "print('denoised    L1 err: ', (torch.norm(denoised_dyn - dyn,p=1)/torch.norm(dyn,p=1)).item())\n",
    "vmax = .5\n",
    "illustrate(dyn, vmin=0,vmax=vmax,title='ground truth dynamics',   time_pts=time_pts)\n",
    "illustrate(noisy_dyn, vmin=0,vmax=vmax,title='noisy dynamics',   time_pts=time_pts)\n",
    "illustrate(denoised_dyn,vmin=0,vmax=vmax,title='denoised dynamics',time_pts=time_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4f67f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "truemass = compute_mass(dyn)\n",
    "further_denoised_dyn = postprocessor(noisy_dyn, truemass,\\\n",
    "                                     lr=1e-5,\\\n",
    "                                     weight_datafid=0, weight_masscon=1e2, weight_TVA=1e-4,\\\n",
    "                                     dyn=dyn,\\\n",
    "                                     maxIter=1e4,\\\n",
    "                                     print_every=500) # denoised_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8331f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot line profile\n",
    "line_index = 0\n",
    "plt_range =  320\n",
    "\n",
    "plt_type = 1\n",
    "\n",
    "nrows=4; ncols=2\n",
    "figsize=(9,18); fontsize=13\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=False, figsize=figsize)\n",
    "for t in range(dep): \n",
    "    plt.rcParams.update({'font.size': fontsize})\n",
    "    if plt_type == 1: # horizontal line profile\n",
    "        axs[t//ncols][t%ncols].plot(dyn[0,0,t,:plt_range,line_index], 'r', label='ground truth')\n",
    "        axs[t//ncols][t%ncols].plot(noisy_dyn[0,0,t,:plt_range,line_index], 'k', label='noisy data')\n",
    "#         plt.plot(denoised_dyn[0,0,time_index,:plt_range,line_index], 'g--', linewidth=2, label='denoised data')\n",
    "        axs[t//ncols][t%ncols].plot(further_denoised_dyn[0,0,t,:plt_range,line_index], '--', color='#00FF00',linewidth=3 ,label='denoised data')\n",
    "    elif plt_type == 2: # vertical line profile\n",
    "        plt.plot(dyn[0,0,t,line_index,:plt_range], 'r',          label='ground truth')\n",
    "        plt.plot(noisy_dyn[0,0,t,line_index,:plt_range], 'k',    label='noisy data')\n",
    "#         plt.plot(denoised_dyn[0,0,time_index,line_index,:plt_range], 'g--', linewidth=2, label='denoised data')\n",
    "        plt.plot(further_denoised_dyn[0,0,t,line_index,:plt_range], label='denoised data')\n",
    "    elif plt_type == 3: # diagonal line profile\n",
    "        gt = [dyn[0,0,t,i,i] for i in range(plt_range)]\n",
    "        noisy_sg = [noisy_dyn[0,0,t,i,i] for i in range(plt_range)]\n",
    "        denoised_sg = [further_denoised_dyn[0,0,t,i,i] for i in range(plt_range)]\n",
    "        plt.plot(gt, 'r', label='ground truth')\n",
    "        plt.plot(noisy_sg, 'k', label='noisy data')\n",
    "        plt.plot(denoised_sg, 'g--', linewidth=2, label='denoised data')\n",
    "    axs[t//ncols][t%ncols].set_title('t = ' + str(time_pts[t].item()))\n",
    "#     axs[t//ncols][t%ncols].legend(loc='best')\n",
    "    \n",
    "\n",
    "handles, labels =  axs[3][1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels=labels,loc=\"lower center\",ncol=3,fontsize='large',bbox_to_anchor=[.5, .06])\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "dir_fig = '/home/leo/hydro/figures/line_profiles_type2_abelgaussian_scaling1.eps'\n",
    "plt.savefig(dir_fig,format='eps',dpi=600,transparent=True,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot line profile\n",
    "line_index = 0\n",
    "plt_range =  320\n",
    "\n",
    "plt_type = 1\n",
    "\n",
    "for time_index in range(dep): \n",
    "    plt.figure(figsize=(5,5))\n",
    "    if plt_type == 1: # horizontal line profile\n",
    "        plt.plot(dyn[0,0,time_index,:plt_range,line_index], 'r', label='ground truth')\n",
    "        plt.plot(noisy_dyn[0,0,time_index,:plt_range,line_index], 'k', label='noisy data')\n",
    "#         plt.plot(denoised_dyn[0,0,time_index,:plt_range,line_index], 'g--', linewidth=2, label='denoised data')\n",
    "        plt.plot(further_denoised_dyn[0,0,time_index,:plt_range,line_index], '--', color='#00FF00',linewidth=3 ,label='post-processed data')\n",
    "    elif plt_type == 2: # vertical line profile\n",
    "        plt.plot(dyn[0,0,time_index,line_index,:plt_range], 'r',          label='ground truth')\n",
    "        plt.plot(noisy_dyn[0,0,time_index,line_index,:plt_range], 'k',    label='noisy data')\n",
    "#         plt.plot(denoised_dyn[0,0,time_index,line_index,:plt_range], 'g--', linewidth=2, label='denoised data')\n",
    "        plt.plot(further_denoised_dyn[0,0,time_index,line_index,:plt_range], label='post-processed data')\n",
    "    elif plt_type == 3: # diagonal line profile\n",
    "        gt = [dyn[0,0,time_index,i,i] for i in range(plt_range)]\n",
    "        noisy_sg = [noisy_dyn[0,0,time_index,i,i] for i in range(plt_range)]\n",
    "        denoised_sg = [further_denoised_dyn[0,0,time_index,i,i] for i in range(plt_range)]\n",
    "        plt.plot(gt, 'r', label='ground truth')\n",
    "        plt.plot(noisy_sg, 'k', label='noisy data')\n",
    "        plt.plot(denoised_sg, 'g--', linewidth=2, label='denoised data')\n",
    "    plt.title(f'time = {time_pts[time_index]}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0096dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a simple animation\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "# plt.figure()\n",
    "# for i in range(dep):\n",
    "#     plt.imshow(dyn[0,0,i,:,:],origin='lower')\n",
    "#     plt.colorbar()\n",
    "#     plt.clim(0, .8)\n",
    "# #     plt.title('Frame %d' % (i+1))\n",
    "#     plt.title('Frame %d' % (i+1+dep*subgroup))\n",
    "# #     plt.savefig(f'/home/huangz78/checkpoints/frame_{i}.jpg',bbox_inches='tight', transparent=True,\n",
    "# #                pad_inches=.2, dpi=300)\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ef24d",
   "metadata": {},
   "source": [
    "## save figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31016b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vmax_val = .5\n",
    "dir_fig = '/home/leo/hydro/figures/'\n",
    "illustrate(dyn, vmin=0, vmax=vmax_val,title_on=False,time_pts=time_pts,\\\n",
    "           save_path=dir_fig+'gt_type2.eps',dpi=150,nrows=4,ncols=2,figsize=(9,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2def23d",
   "metadata": {},
   "source": [
    "## make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_rec1 = '/home/leo/hydro/hist_Abel-gaussian_1.npz'\n",
    "dir_rec2 = '/home/leo/hydro/hist_Abel-gaussian_0.3.npz'\n",
    "dir_rec3 = '/home/leo/hydro/hist_Abel-gaussian-double_1.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1 = np.load(dir_rec1)\n",
    "massdiff_gauss_1, nrmse_gauss_1, nl1err_gauss_1 = rec1['massdiff'], rec1['nrmse'], rec1['nl1err']\n",
    "rec2 = np.load(dir_rec2)\n",
    "massdiff_gauss_p3, nrmse_gauss_p3, nl1err_gauss_p3 = rec2['massdiff'], rec2['nrmse'], rec2['nl1err']\n",
    "rec3 = np.load(dir_rec1)\n",
    "massdiff_gauss_double, nrmse_gauss_double, nl1err_gauss_double = rec3['massdiff'], rec3['nrmse'], rec3['nl1err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f5a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23fabe4",
   "metadata": {},
   "source": [
    "## Does netD tells noisy dynamics from clean dynamics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load slices\n",
    "ngpu = 1\n",
    "traintotal = 1000\n",
    "labels_gt = list([])\n",
    "labels_pred = list([])\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "use_cuda = True if (torch.cuda.is_available() and ngpu > 0) else False\n",
    "datapath = '/mnt/DataB/hydro_simulations/data/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".nc\"):\n",
    "        ncfiles.append(file)\n",
    "print('Total amount of available files:', len(ncfiles))\n",
    "print('Train file amount: {}'.format(traintotal))\n",
    "\n",
    "filestart  = 8000\n",
    "trainfiles = ncfiles[filestart:filestart+traintotal+800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e33bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnet.eval()\n",
    "device = torch.device('cuda:0')\n",
    "dnet.to(device)\n",
    "b_size = 5;  dep = 8; img_size = 320; resize_option = False; noise_mode = 'real'; normalize_factor = 50\n",
    "with torch.no_grad():\n",
    "    fileind = 0          \n",
    "    while fileind < traintotal:\n",
    "        # set the model back to training mode\n",
    "        dyn, noise = load_data_batch(fileind, trainfiles, \\\n",
    "                                     b_size=b_size, dep=dep, img_size=img_size,\\\n",
    "                                     resize_option=resize_option,\\\n",
    "                                     noise_mode=noise_mode, normalize_factor=normalize_factor)\n",
    "        noise    = noise.to(device)\n",
    "        real_cpu = dyn.to(device)\n",
    "\n",
    "        label_real = torch.full((b_size,), 1., dtype=torch.float, device=device)\n",
    "        labels_gt.extend(list(label_real.flatten().cpu().numpy()))\n",
    "        label_pred = torch.round(dnet(real_cpu).detach())\n",
    "        labels_pred.extend(list(label_pred.flatten().cpu().numpy()))\n",
    "        \n",
    "        label_fake = torch.full((b_size,), 0., dtype=torch.float, device=device)\n",
    "        labels_gt.extend(list(label_fake.flatten().cpu().numpy()))\n",
    "        label_pred = torch.round(dnet(real_cpu+noise).detach())\n",
    "        labels_pred.extend(list(label_pred.flatten().cpu().numpy()))\n",
    "        \n",
    "        print(f'current prediction accuracy = {1 - (torch.tensor(labels_pred)- torch.tensor(labels_gt)).abs().sum().item() / len(labels_gt)}')\n",
    "        fileind += b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(labels_gt, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019957d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4eeef0b",
   "metadata": {},
   "source": [
    "# Is denoising in the Fourier domain a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24734c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = torch.tensor(np.linspace(0,2*np.pi,256))\n",
    "y  = torch.sin(x)\n",
    "z  = torch.ones_like(x) * 0.5\n",
    "yy = y + z\n",
    "\n",
    "f  = F.fft(y)\n",
    "ff = F.fft(yy)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.abs(f[0:50]),label='f')\n",
    "# plt.plot(torch.abs(ff),label='ff')\n",
    "plt.legend(loc='best')\n",
    "# plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(torch.abs(f),label='f')\n",
    "plt.plot(torch.abs(ff[0:50]),label='ff')\n",
    "plt.legend(loc='best')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19966836",
   "metadata": {},
   "source": [
    "# check conservation of mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/mnt/shared_b/data/hydro_simulations/data/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".nc\"):\n",
    "        ncfiles.append(file)\n",
    "print('Total amount of files:', len(ncfiles))\n",
    "img_size = 320\n",
    "dep = 41\n",
    "\n",
    "batchsize = 5\n",
    "fileexp_inds = [7736+i for i in range(batchsize)]\n",
    "\n",
    "dyn   = torch.zeros((batchsize,1,dep,img_size,img_size))    # load one sample, show it\n",
    "noise = torch.zeros((batchsize,1,dep,img_size,img_size))    # make a noise sample, add it to the ground truth, show it\n",
    "\n",
    "for fileexp_ind in range(len(fileexp_inds)):\n",
    "    filename = ncfiles[fileexp_inds[fileexp_ind]]\n",
    "    sim = xr.open_dataarray(datapath+filename)\n",
    "    # val.append(np.max(np.array(sim)[:dep,:,:].flatten()))\n",
    "    sim.close()\n",
    "    for t in range(dep):\n",
    "        dyn[fileexp_ind,0,t,:,:] = torch.tensor( sim.isel(t=t)[:img_size,:img_size].values )\n",
    "\n",
    "# dyn   = np.zeros((1,1,dep,256,256))    # load one sample, show it\n",
    "# noise = np.zeros((1,1,dep,256,256))    # make a noise sample, add it to the ground truth, show it\n",
    "# for t in range(dep):\n",
    "#     dyn[0,0,t,:,:] = resize(sim.isel(t=t)[:img_size,:img_size].values,(256,256),anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551d543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check conservation of mass\n",
    "\n",
    "mass = np.zeros(dep)\n",
    "for ind in range(dep):\n",
    "    img = dyn[0,0,ind,:,:]\n",
    "    mass[ind] = integration_over_sphere(img)\n",
    "plt.scatter(range(dep),mass)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dyn[0,0,0,:,:]\n",
    "plt.imshow(img,origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce6c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mass(imgs,Rrho=1,Rz=1):\n",
    "    '''\n",
    "    computing through cylindrical coordinate\n",
    "    '''\n",
    "    drho = Rrho / imgs.shape[3]\n",
    "    dz   = Rz   / imgs.shape[4]\n",
    "    metrics = torch.linspace(0,Rrho,imgs.shape[4]).repeat(imgs.shape[3],1)\n",
    "    integrand = imgs * metrics\n",
    "    mass = 2*np.pi * torch.sum(integrand,dim=(3,4)) * drho * dz \n",
    "    return torch.squeeze(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34416472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_mass(dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dddfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = np.zeros(dep)\n",
    "for ind in range(dep):\n",
    "    img = dyn[0,0,ind,:,:]\n",
    "    mass[ind] = compute_mass(img)\n",
    "plt.scatter(range(dep),mass)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707268d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mass(rho, R, z):\n",
    "    dR = R[1] - R[0]\n",
    "    dz = z[1] - z[0]\n",
    "    m = sum(2*np.pi*R[:]*rho[:])*dR*dz\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_r, grid_theta = np.mgrid[0:1:301j, np.pi/2:np.pi:301j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8796dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grid_r.T,origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('r grid')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(grid_theta.T,origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('theta grid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adec832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_over_sphere(img,img_size=320,grid_len=300,method='cubic',R=1,Rx=1,Ry=1):\n",
    "    \n",
    "    grid_r, grid_theta = np.mgrid[0:R:301j, np.pi/2:np.pi:301j]\n",
    "    dr = 1/grid_len\n",
    "    dtheta = np.pi/(2*grid_len)\n",
    "    Cartesian_grid = [[i/img_size*Rx,j/img_size*Ry] for i in range(img_size) for j in range(img_size)]\n",
    "    radius = lambda x,y: np.sqrt(x**2 + y**2)\n",
    "    theta  = lambda x,y: np.arctan(x/y) + np.pi/2\n",
    "    \n",
    "    Spherical_grid = []\n",
    "    vals = []\n",
    "    for point in Cartesian_grid:\n",
    "        if point[0]==0:\n",
    "            Spherical_grid.append([point[1],np.pi/2])\n",
    "        elif (point[1]==0) and (point[0]!=0):\n",
    "            Spherical_grid.append([point[0],np.pi])\n",
    "        else:\n",
    "            Spherical_grid.append([radius(point[0],point[1]),theta(point[0],point[1])])\n",
    "        vals.append(img[int(point[0]*img_size/Rx),int(point[1]*img_size/Ry)])\n",
    "        \n",
    "    Spherical_grid = np.array(Spherical_grid)\n",
    "#     Cartesian_grid = np.array(Cartesian_grid)\n",
    "    vals = np.array(vals)\n",
    "\n",
    "    den_interp = griddata(Spherical_grid, vals, (grid_r, grid_theta), method=method).T\n",
    "    metric = (grid_r.T)**2 * np.sin(grid_theta.T)\n",
    "    \n",
    "    mass = np.sum(metric * np.nan_to_num(den_interp)) * dr * dtheta\n",
    "    \n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Spherical_grid[:,0],Spherical_grid[:,1],c=vals)\n",
    "plt.xlabel('radius')\n",
    "plt.ylabel('theta')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(den_interp.T, extent=(0,1,np.pi/2,np.pi), origin='lower')\n",
    "plt.title('Cubic interpolation')\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.colorbar()\n",
    "plt.xlabel('radius')\n",
    "plt.ylabel('theta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Spherical_grid[:,0],Spherical_grid[:,1],vals)\n",
    "ax.set_xlabel('r')\n",
    "ax.set_ylabel('theta')\n",
    "ax.set_zlabel('val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ad17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = np.loadtxt('/home/huangz78/hydro/leo1_trada.txt')\n",
    "heg = 860\n",
    "wid = 880\n",
    "dx = 2.558140 * 1e-2 \n",
    "dy = 2.500000 * 1e-2\n",
    "dyn = np.reshape(dyn,(heg,wid))\n",
    "Rx = dx * 320\n",
    "Ry = dy * 320\n",
    "R  = min(Rx,Ry)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(dyn)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgslice = dyn[heg//2:heg//2+320,wid//2:wid//2+320]\n",
    "plt.imshow(imgslice)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a136c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_over_sphere(imgslice,R=R,Rx=Rx,Ry=Ry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a90f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heg = 860\n",
    "wid = 880\n",
    "dx = 2.558140 * 1e-2 \n",
    "dy = 2.500000 * 1e-2\n",
    "\n",
    "weights = np.array([[i**2+j**2 for j in (np.arange(0,wid)-wid//2)*dy] for i in (np.arange(0,heg)-heg//2)*dx])\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(weights)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "summation = np.sum(np.multiply(weights,dyn))*(dx*dy)*4/3*np.pi\n",
    "print(summation)\n",
    "\n",
    "# summation = np.zeros((dep))\n",
    "# for t in range(dep):\n",
    "#     summation[t] = np.sum(np.multiply(weights,dyn[0,0,t,:,:]))\n",
    "# plt.figure()\n",
    "# plt.scatter(range(dep),summation)\n",
    "# plt.xlabel('frame number')\n",
    "# plt.ylabel('weighted summation of pixels')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771453d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = np.array([[i**2+j**2 for i in np.arange(0,img_size)/img_size] for j in np.arange(0,img_size)/img_size])\n",
    "plt.figure()\n",
    "plt.imshow(weights)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "summation = np.zeros((dep))\n",
    "for t in range(dep):\n",
    "    summation[t] = np.sum(np.multiply(weights,dyn[0,0,t,:,:]))\n",
    "plt.figure()\n",
    "plt.scatter(range(dep),summation)\n",
    "plt.xlabel('frame number')\n",
    "plt.ylabel('weighted summation of pixels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da761c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a9c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e61a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import imageio\n",
    "def create_gif(filenames, duration):\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        images.append(imageio.imread(filename))\n",
    "    output_file = '/home/huangz78/checkpoints/Gif-%s.gif' % datetime.datetime.now().strftime('%Y-%M-%d-%H-%M-%S')\n",
    "    imageio.mimsave(output_file, images, duration=duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=[f'/home/huangz78/checkpoints/frame_{i}.jpg' for i in range(41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3feb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif(filenames, .4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
